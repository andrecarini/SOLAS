{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# SOLAS Results Analysis Notebook\n\nThis notebook analyzes and visualizes evaluation results from completed SOLAS experiments.\n\n**Prerequisites:** You must have already run experiments using the Evaluation notebook and have results saved.\n\n**Note:** This notebook does NOT require a GPU - analysis runs entirely on CPU.\n\n## Analyses Available\n\n| Analysis | Description |\n|----------|-------------|\n| ASR Model Comparison | Compare Whisper tiny/small/large transcription quality |\n| Quantization Impact | Compare 4-bit quantization vs full precision |\n| Repetition Penalty Impact | Analyze degeneration prevention |\n| Summary Mode Impact | Compare greedy/sampled summarization |\n| Chunk Size Impact | Evaluate text chunking strategies |\n| Temperature Impact | Analyze creativity settings for podcast generation |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# @title ### Setup\n",
    "# @markdown Clone/update SOLAS repository and load evaluation results.\n",
    "# @markdown **No GPU required** - this notebook only analyzes existing results.\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Clone/update SOLAS repository\n",
    "if Path('SOLAS').exists():\n",
    "    subprocess.run(['git', 'pull'], check=True, cwd='SOLAS')\n",
    "else:\n",
    "    subprocess.run(['git', 'clone', 'https://github.com/andrecarini/SOLAS.git'], check=True)\n",
    "\n",
    "sys.path.insert(0, 'SOLAS')\n",
    "\n",
    "# Create evaluation interface (handles Google Drive mounting automatically)\n",
    "from library import EvaluationNotebook\n",
    "evaluation = EvaluationNotebook(\n",
    "    solas_dir=None,                      # Auto-detect: /content/SOLAS (Colab) or ./SOLAS (local)\n",
    "    use_gdrive=None,                     # Auto-detect: True in Colab, False otherwise\n",
    "    gdrive_mount_point='/gdrive',        # Where to mount Google Drive\n",
    "    gdrive_folder='SOLAS',               # Folder name in Google Drive MyDrive\n",
    "    gdrive_symlink='/content/gdrive',    # Symlink path for easy access\n",
    "    local_dir='./evaluation_results'     # Local directory when not using Google Drive\n",
    ")\n",
    "evaluation.print_setup_info()\n",
    "\n",
    "# Load and display results summary\n",
    "results = evaluation.load_results()\n",
    "if not results.get('experiments'):\n",
    "    print('\\n\\u26a0\\ufe0f No evaluation results found.')\n",
    "    print('Run the Evaluation notebook first to generate results.')\n",
    "else:\n",
    "    print(f'\\n\\u2705 Loaded {len(results[\"experiments\"])} experiments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# @title ### ASR Model Comparison\n",
    "# @markdown Compare Whisper tiny/small/large-v3 transcription results with visual table and scrollable transcript views.\n",
    "# @markdown Includes degeneration detection - models with severe repetition loops are marked as FAIL.\n",
    "\n",
    "evaluation.asr_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# @title ### Quantization Impact\n",
    "# @markdown Compare 4-bit quantization vs full precision across all LLMs.\n",
    "# @markdown Shows text outputs, metrics tables, and comparison summary.\n",
    "\n",
    "evaluation.quantization_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# @title ### Repetition Penalty Impact\n",
    "# @markdown Compare repetition penalty (None vs 1.2) on Qwen2-0.5B and Mistral-7B.\n",
    "# @markdown Shows text outputs, metrics tables, and overall results.\n",
    "\n",
    "evaluation.repetition_penalty_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "collapsed": true
   },
   "outputs": [],
   "source": "# @title ### Summary Mode Impact\n# @markdown Compare greedy and sampled summary modes on Phi-3-mini (no quantization).\n# @markdown Note: Summary mode only affects the summary stage.\n\nevaluation.summary_mode_analysis()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# @title ### Chunk Size Impact\n",
    "# @markdown Compare 2000 vs 4000 character chunks on Phi-3-mini (no quantization).\n",
    "# @markdown Shows how chunk size affects translation, summary, and podcast generation.\n",
    "\n",
    "evaluation.chunk_size_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "collapsed": true
   },
   "outputs": [],
   "source": "# @title ### Temperature Impact\n# @markdown Compare temperature 0.2/0.5 on Mistral-7B podcast generation.\n# @markdown Temperature only affects the podcast stage (translation and summary use greedy decoding).\n\nevaluation.temperature_analysis()"
  }
 ]
}