{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# SOLAS Automated Evaluation Notebook\n\nThis notebook runs controlled experiments to evaluate SOLAS pipeline performance across different configurations.\n\n## Design Principles\n\n1. **Controlled experiments**: Vary ONE parameter at a time while holding others fixed\n2. **Config-based caching**: Hash configurations (including hardware runtime) to detect duplicates and skip re-runs\n3. **Resumable**: Automatically resumes from last completed experiment after runtime restart\n4. **Persistent storage**: All data saved to Google Drive immediately after each experiment\n5. **Comprehensive metrics**: Time, RAM, VRAM, hardware info, full inputs/outputs for every stage\n6. **Model lifecycle management**: Explicit load/unload to ensure accurate memory measurements\n\n## Experiments\n\n| Experiment | Purpose | Tests |\n|------------|---------|-------|\n| ASR Models | Compare Whisper tiny/small/large | 3 |\n| Quantization | None vs 4-bit on all LLMs | 8 |\n| Repetition Penalty | None vs 1.2 on smallest and largest LLMs | 4 |\n| Summary Mode | greedy/sampled | 2 |\n| Chunk Size | 2000 vs 4000 chars | 2 |\n| Temperature | 0.2/0.5 on Mistral-7B | 2 |\n| **Total** | | **21** |\n\n## Results Analysis\n\nAfter running experiments, use the **[SOLAS_Analysis.ipynb](SOLAS_Analysis.ipynb)** notebook to visualize and analyze results.\nThe Analysis notebook does not require a GPU and can be run on any runtime."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# @title ### Setup & Engine\n",
    "# @markdown Initialize environment, clone SOLAS repository, and configure evaluation system.\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Clone/update SOLAS repository\n",
    "if Path('SOLAS').exists():\n",
    "    subprocess.run(['git', 'pull'], check=True, cwd='SOLAS')\n",
    "else:\n",
    "    subprocess.run(['git', 'clone', 'https://github.com/andrecarini/SOLAS.git'], check=True)\n",
    "\n",
    "sys.path.insert(0, 'SOLAS')\n",
    "\n",
    "# Check environment and setup dependencies\n",
    "from library import check_colab_environment, setup_environment_with_progress\n",
    "check_colab_environment()\n",
    "setup_result = setup_environment_with_progress()\n",
    "\n",
    "# Set global flags for other cells to check\n",
    "RESTART_NEEDED = setup_result.get('restart_needed', False)\n",
    "SETUP_COMPLETE = True\n",
    "\n",
    "# Create evaluation interface (handles Google Drive mounting automatically)\n",
    "from library import EvaluationNotebook\n",
    "evaluation = EvaluationNotebook(\n",
    "    solas_dir=None,                      # Auto-detect: /content/SOLAS (Colab) or ./SOLAS (local)\n",
    "    use_gdrive=None,                     # Auto-detect: True in Colab, False otherwise\n",
    "    gdrive_mount_point='/gdrive',        # Where to mount Google Drive\n",
    "    gdrive_folder='SOLAS',               # Folder name in Google Drive MyDrive\n",
    "    gdrive_symlink='/content/gdrive',    # Symlink path for easy access\n",
    "    local_dir='./evaluation_results'     # Local directory when not using Google Drive\n",
    ")\n",
    "evaluation.print_setup_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# @title ### Run All Experiments\n",
    "# @markdown Execute all remaining experiments. Safe to re-run - automatically skips completed experiments and duplicates.\n",
    "\n",
    "# ============================================================================\n",
    "# Run basic checks\n",
    "# ============================================================================\n",
    "if 'RESTART_NEEDED' in globals() and RESTART_NEEDED:\n",
    "    from library import show_restart_warning\n",
    "    show_restart_warning()\n",
    "\n",
    "if 'SETUP_COMPLETE' not in globals() or not SETUP_COMPLETE:\n",
    "    raise RuntimeError(\"Setup not completed. Please run the first cell to complete environment setup.\")\n",
    "\n",
    "# ============================================================================\n",
    "# Execute evaluation\n",
    "# ============================================================================\n",
    "evaluation.run_evaluation(dry_run=False);  # Semicolon suppresses results output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# @title ### View Results\n",
    "# @markdown Display evaluation results summary with metrics and completion status.\n",
    "\n",
    "# ============================================================================\n",
    "# Run basic checks\n",
    "# ============================================================================\n",
    "if 'RESTART_NEEDED' in globals() and RESTART_NEEDED:\n",
    "    from library import show_restart_warning\n",
    "    show_restart_warning()\n",
    "\n",
    "if 'SETUP_COMPLETE' not in globals() or not SETUP_COMPLETE:\n",
    "    raise RuntimeError(\"Setup not completed. Please run the first cell to complete environment setup.\")\n",
    "\n",
    "# ============================================================================\n",
    "# Display results\n",
    "# ============================================================================\n",
    "evaluation.display_results()"
   ]
  }
 ]
}